# tools/LLM/run_gpt_prompt.py (æ—¶é—´æ­£åˆ™æœ€ç»ˆå®Œæ•´ç‰ˆ)

import json
import re
import os
import sys
import traceback
import random
from datetime import datetime
from .ollama_agent import OllamaAgent
import asyncio
from opencc import OpenCC

# --- å…¨å±€é…ç½®ä¸æ—¥å¿— ---
try:
    cc = OpenCC('s2twp')
    print("âœ… [SUCCESS] OpenCC ç®€ç¹è½¬æ¢å™¨åˆå§‹åŒ–æˆåŠŸã€‚")
except Exception as e:
    print(f"âŒ [CRITICAL_ERROR] åˆå§‹åŒ– OpenCC å¤±è´¥: {e}", file=sys.stderr)
    class MockCC:
        def convert(self, text): return text
    cc = MockCC()

OLLAMA_URL = "http://127.0.0.1:11434/api"
MODEL_NAME = "deepseek-r1:14b"
PROMPT_DIR = os.path.join(os.path.dirname(__file__), 'prompt_template')
os.makedirs(PROMPT_DIR, exist_ok=True)

LLM_LOG_BUFFER = []
MAX_LLM_LOG_LINES = 200

def log_llm_call(prompt_key, final_prompt, raw_response, final_output):
    global LLM_LOG_BUFFER
    log_entry = (f"--- LLM Call @ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ---\n"
                 f"Prompt Key: {prompt_key}\nFinal Prompt:\n---\n{final_prompt}\n---\n"
                 f"Raw Response:\n---\n{raw_response}\n---\n"
                 f"Final Parsed Output:\n---\n{json.dumps(final_output, ensure_ascii=False, indent=2)}\n"
                 f"---------------------------------------------------\n")
    LLM_LOG_BUFFER.append(log_entry)
    if len(LLM_LOG_BUFFER) > MAX_LLM_LOG_LINES: LLM_LOG_BUFFER.pop(0)

def get_llm_log(): return "\n".join(LLM_LOG_BUFFER)

PROMPT_PATHS = {
    "generate_schedule": os.path.join(PROMPT_DIR, "ç”Ÿæˆæ—¥ç¨‹å®‰æ’æ—¶é—´è¡¨.txt"), "wake_up_hour": os.path.join(PROMPT_DIR, "èµ·åºŠæ—¶é—´.txt"),
    "pronunciatio": os.path.join(PROMPT_DIR, "è¡Œä¸ºè½¬ä¸ºå›¾æ ‡æ˜¾ç¤º.txt"), "double_chat": os.path.join(PROMPT_DIR, "èŠå¤©.txt"),
    "go_map": os.path.join(PROMPT_DIR, "è¡ŒåŠ¨éœ€è¦å»çš„åœ°æ–¹.txt"), "modify_schedule": os.path.join(PROMPT_DIR, "ç»†åŒ–æ¯æ—¥å®‰æ’æ—¶é—´è¡¨.txt"),
    "summarize_chat": os.path.join(PROMPT_DIR, "æ€»ç»“ç»å†äº¤è°ˆä¸ºè®°å¿†.txt"), "get_recovery_action": os.path.join(PROMPT_DIR, "è·å–æ¢å¤è¡Œå‹•.txt"),
    "summarize_disaster": os.path.join(PROMPT_DIR, "æ€»ç»“ç¾å®³ç»å†.txt"), "generate_initial_memory": os.path.join(PROMPT_DIR, "ç”Ÿæˆåˆå§‹è¨˜æ†¶.txt"),
    "inner_monologue": os.path.join(PROMPT_DIR, "ç”Ÿæˆå…§å¿ƒç¨ç™½.txt"), "generate_action_thought": os.path.join(PROMPT_DIR, "ç”Ÿæˆè¡Œå‹•æƒ³æ³•.txt"),
    "earthquake_step_action": os.path.join(PROMPT_DIR, "ç”Ÿæˆåœ°éœ‡ä¸­è¡Œå‹•.txt"), "generate_weekly_schedule": os.path.join(PROMPT_DIR, "ç”Ÿæˆä¸ƒæ—¥è¡Œäº‹æ›†.txt"),
}

ollama_agent = None
async def initialize_llm():
    global ollama_agent
    if ollama_agent is None:
        try:
            ollama_agent = OllamaAgent(MODEL_NAME, OLLAMA_URL)
            print(f"âœ… [SUCCESS] OllamaAgent (æµå¼) åˆå§‹åŒ–æˆåŠŸï¼Œæ¨¡å‹: {MODEL_NAME}")
            return True
        except Exception as e:
            print(f"âŒ [CRITICAL_ERROR] åˆå§‹åŒ– OllamaAgent (æµå¼) å¤±æ•—: {e}", file=sys.stderr)
            return False
    return True

async def close_llm_session():
    if ollama_agent: await ollama_agent.close_session(); print("OllamaAgent ä¼šè¯å·²å…³é—­ã€‚")

def _json_output_regex(text: str, default: any):
    if not isinstance(text, str): return default
    match = re.search(r"```json\s*(\{.*?\})\s*```", text, re.DOTALL)
    json_str = match.group(1) if match else None
    if not json_str:
        start, end = text.find('{'), text.rfind('}')
        if start != -1 and end != -1 and end > start: json_str = text[start:end+1]
        else:
            if isinstance(default, str): return text.strip()
            return default
    try:
        data = json.loads(json_str)
        return data.get("output", data)
    except json.JSONDecodeError:
        if isinstance(default, str): return text.strip()
        return default

async def _safe_llm_call(prompt_key: str, prompt_args: list, special_instruction: str, default_on_error: any):
    if not await initialize_llm(): return default_on_error
    prompt_template_path = PROMPT_PATHS.get(prompt_key)
    if not os.path.exists(prompt_template_path): return default_on_error
    raw_response, final_output, prompt = "N/A", default_on_error, ""
    try:
        processed_args = [str(arg) for arg in prompt_args]
        prompt = OllamaAgent.generate_prompt(processed_args, prompt_template_path)
        final_instruction = f"{special_instruction} è«‹å‹™å¿…ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼ˆTraditional Chineseï¼‰å›ç­”ã€‚"
        is_json_output = not isinstance(default_on_error, str)
        raw_response = await ollama_agent.ollama_stream_generate_response(prompt=prompt, special_instruction=final_instruction, expect_json=is_json_output, example_output=default_on_error)
        if raw_response is None: final_output = default_on_error
        else: final_output = _json_output_regex(raw_response, default_on_error) if is_json_output else raw_response
        if isinstance(final_output, str): converted_output = cc.convert(final_output)
        elif isinstance(final_output, dict): converted_output = {cc.convert(k): (cc.convert(v) if isinstance(v, str) else v) for k, v in final_output.items()}
        elif isinstance(final_output, list):
            def convert_list_items(item):
                if isinstance(item, str): return cc.convert(item)
                if isinstance(item, list): return [convert_list_items(i) for i in item]
                if isinstance(item, dict): return {cc.convert(k): (cc.convert(v) if isinstance(v, str) else v) for k, v in item.items()}
                return item
            converted_output = [convert_list_items(i) for i in final_output]
        else: converted_output = final_output
        log_llm_call(prompt_key, prompt, str(raw_response), converted_output)
        return converted_output
    except Exception as e:
        print(f"âŒ [LLM_CALL_ERROR] åœ¨ LLM å‘¼å« '{prompt_key}' æœŸé–“ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}", file=sys.stderr)
        log_llm_call(prompt_key, prompt, str(e), default_on_error)
        return default_on_error

async def run_gpt_prompt_generate_initial_memory(name, mbti, persona_summary, home):
    default_memory = "è¨˜æ†¶ç”Ÿæˆå¤±æ•—ï¼Œè«‹æª¢æŸ¥LLMé€£ç·šã€‚"
    memory_result = await _safe_llm_call("generate_initial_memory", [name, mbti, persona_summary, home], 'åƒ…è¿”å›æè¿°ä»£ç†äººèƒŒæ™¯æ•…äº‹çš„ç´”æ–‡å­—å­—ä¸²ã€‚', default_memory)
    success = memory_result != default_memory and isinstance(memory_result, str)
    return str(memory_result), success

async def run_gpt_prompt_generate_weekly_schedule(persona_summary):
    default_schedule = {day: "è‡ªç”±æ´»å‹•" for day in ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"]}
    schedule = await _safe_llm_call("generate_weekly_schedule", [persona_summary], 'è¿”å›ä¸€å€‹åŒ…å«ä¸ƒå¤©ï¼ˆMonday-Sundayï¼‰éµçš„ JSON ç‰©ä»¶ã€‚', default_schedule)
    success = schedule != default_schedule and isinstance(schedule, dict) and len(schedule) == 7
    return schedule, success

async def run_gpt_prompt_generate_hourly_schedule(persona, now_time, today_goal):
    default_on_error = [["è‡ªç”±æ´»å‹•", 1440]]
    return await _safe_llm_call("generate_schedule", [persona, now_time, today_goal], 'è¿”å›ä¸€å€‹åˆ—è¡¨ï¼Œå…¶ä¸­æ¯å€‹å­åˆ—è¡¨åŒ…å«[æ´»å‹•åç¨±, æŒçºŒåˆ†é˜æ•¸]ã€‚', default_on_error)

async def run_gpt_prompt_wake_up_hour(persona, now_time, hourly_schedule):
    schedule_str = json.dumps(hourly_schedule, ensure_ascii=False)
    default_on_error = f"{random.randint(6, 8):02d}-{random.choice(['00', '15', '30'])}"
    raw_llm_output = await _safe_llm_call("wake_up_hour", [persona, now_time, schedule_str], 'è¿”å› "HH:MM" æˆ– "HH-MM" æ ¼å¼çš„æ™‚é–“å­—ä¸²ã€‚', default_on_error)
    match = re.search(r'\b([0-1][0-9]|2[0-3])[:|-]([0-5][0-9])\b', str(raw_llm_output))
    return f"{match.group(1)}-{match.group(2)}" if match else default_on_error

async def run_gpt_prompt_pronunciatio(action_dec):
    common_emojis = {"ç¡è¦º": "ğŸ˜´", "ä¼‘æ¯": "ğŸ›‹ï¸", "åƒé£¯": "ğŸ•", "èŠå¤©": "ğŸ’¬", "å·¥ä½œ": "ğŸ’¼", "å­¸ç¿’": "ğŸ“š", "é†’ä¾†": "â˜€ï¸", "Unconscious": "ğŸ˜µ", "åˆå§‹åŒ–ä¸­": "â³"}
    action_str = str(action_dec)
    for key, emoji in common_emojis.items():
        if key in action_str: return emoji
    return await _safe_llm_call("pronunciatio", [action_str], 'åªè¿”å›ä¸€å€‹æœ€é©åˆçš„ emoji åœ–æ¨™å­—ä¸²ã€‚', "â“")

async def generate_action_thought(persona_summary, current_place, new_action):
    return await _safe_llm_call("generate_action_thought", [persona_summary, current_place, new_action], 'è¿”å›ä¸€å¥ç´„20å­—çš„ç°¡çŸ­å…§å¿ƒæƒ³æ³•å­—ä¸²ã€‚', "")

async def run_gpt_prompt_earthquake_step_action(persona_summary, health, mental_state, current_place, intensity, disaster_log):
    default_on_error = {"action": "ä¿æŒè­¦æƒ•", "thought": "(ææ‡¼ä¸­...)"}
    log_context = "\n".join(disaster_log)
    parsed_output = await _safe_llm_call("earthquake_step_action", [persona_summary, health, mental_state, current_place, intensity, log_context], 'è¼¸å‡ºåŒ…å« "action" å’Œ "thought" éµçš„ JSON ç‰©ä»¶ã€‚', default_on_error)
    return parsed_output.get("action", default_on_error["action"]), parsed_output.get("thought", default_on_error["thought"])

async def double_agents_chat(chat_context):
    default_on_error = {"thought": "è§£æéŒ¯èª¤ã€‚", "dialogue": []}
    prompt_args = [
        chat_context['location'], chat_context['agent1']['name'], chat_context['agent1']['mbti'], chat_context['agent1']['persona'], str(chat_context['agent1']['memory'])[-500:],
        chat_context['agent2']['name'], chat_context['agent2']['mbti'], chat_context['agent2']['persona'], str(chat_context['agent2']['memory'])[-500:],
        chat_context['now_time'], chat_context['agent1']['action'], chat_context['agent2']['action'],
        chat_context.get('eq_ctx') or "ç›®å‰ä¸€åˆ‡æ­£å¸¸ã€‚", json.dumps(chat_context['history'], ensure_ascii=False) ]
    parsed_output = await _safe_llm_call("double_chat", prompt_args, 'è¼¸å‡ºä¸€å€‹åŒ…å« "thought" å’Œ "dialogue" éµçš„ JSON ç‰©ä»¶ã€‚', default_on_error)
    return parsed_output.get("thought", default_on_error["thought"]), parsed_output.get("dialogue", default_on_error["dialogue"])

async def generate_inner_monologue(agent_context):
    default_on_error = {"thought": "è§£æéŒ¯èª¤ã€‚", "monologue": "ï¼ˆæ­£åœ¨æ€è€ƒ...ï¼‰"}
    prompt_args = [
        agent_context['name'], agent_context['mbti'], agent_context['persona'], agent_context['location'],
        agent_context['action'], agent_context['now_time'], str(agent_context['memory'])[-500:],
        agent_context.get('eq_ctx') or "ç›®å‰ä¸€åˆ‡æ­£å¸¸ã€‚" ]
    parsed_output = await _safe_llm_call("inner_monologue", prompt_args, 'è¼¸å‡ºä¸€å€‹åŒ…å« "thought" å’Œ "monologue" éµçš„ JSON ç‰©ä»¶ã€‚', default_on_error)
    return parsed_output.get("thought", default_on_error["thought"]), parsed_output.get("monologue", default_on_error["monologue"])
    
async def run_gpt_prompt_summarize_disaster(agent_name, mbti, health, experience_log):
    log_str = "\n".join([str(entry) for entry in experience_log]) or "(æ²’æœ‰å…·é«”äº‹ä»¶è¨˜éŒ„)"
    return await _safe_llm_call("summarize_disaster", [agent_name, mbti, health, log_str], 'è¿”å›ç°¡çŸ­çš„ç½å¾Œè¨˜æ†¶ç¸½çµå­—ä¸²ã€‚', "ç¶“æ­·äº†ä¸€å ´åœ°éœ‡ï¼Œç¾åœ¨å®‰å…¨ã€‚")

async def run_gpt_prompt_get_recovery_action(persona, mental_state, curr_place):
    return await _safe_llm_call("get_recovery_action", [persona, mental_state, curr_place], 'è¿”å›å»ºè­°çš„æ¢å¾©è¡Œå‹•çŸ­èªå­—ä¸²ã€‚', "åŸåœ°ä¼‘æ¯")