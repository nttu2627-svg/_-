# tools/LLM/run_gpt_prompt.py (æ—¶é—´æ­£åˆ™æœ€ç»ˆå®Œæ•´ç‰ˆ)

import json
import re
import os
import sys
import traceback
import random
from datetime import datetime
from .ollama_agent import OllamaAgent
import asyncio
from opencc import OpenCC

# --- å…¨å±€é…ç½®ä¸æ—¥å¿— ---
try:
    cc = OpenCC('s2twp')
    print("âœ… [SUCCESS] OpenCC ç¹ç°¡è½‰æ›å™¨åˆå§‹åŒ–æˆåŠŸã€‚")
except Exception as e:
    print(f"âŒ [CRITICAL_ERROR] åˆå§‹åŒ– OpenCC å¤±è´¥: {e}", file=sys.stderr)
    class MockCC:
        def convert(self, text): return text
    cc = MockCC()

OLLAMA_URL = "http://127.0.0.1:11434/api"
MODEL_NAME = "deepseek-r1:14b"
PROMPT_DIR = os.path.join(os.path.dirname(__file__), 'prompt_template')
os.makedirs(PROMPT_DIR, exist_ok=True)
COMMON_EMOJIS = {
    "ç¡è¦º": "ğŸ˜´",
    "ä¼‘æ¯": "ğŸ›‹ï¸",
    "åƒé£¯": "ğŸ•",
    "èŠå¤©": "ğŸ’¬",
    "å·¥ä½œ": "ğŸ’¼",
    "å­¸ç¿’": "ğŸ“š",
    "é†’ä¾†": "â˜€ï¸",
    "Unconscious": "ğŸ˜µ",
    "åˆå§‹åŒ–ä¸­": "â³",
    "ç§»å‹•ä¸­": "ğŸ‘Ÿ",
    "æœªçŸ¥": "â“",
}
ALLOWED_EMOJIS = set(COMMON_EMOJIS.values())
LLM_LOG_BUFFER = []
MAX_LLM_LOG_LINES = 400

def _limit_repetitive_sequences(text: str, max_repeat: int = 6, max_seq_len: int = 12):
    """Clamp pathological repetitions caused by LLM streaming glitches."""
    if not isinstance(text, str) or not text:
        return text, False

    sanitized = text
    changed = False
    for seq_len in range(1, max_seq_len + 1):
        pattern = re.compile(rf'(.{{{seq_len}}})\1{{{max_repeat},}}', flags=re.DOTALL)

        def _replace(match):
            nonlocal changed
            changed = True
            segment = match.group(1)
            return segment * max_repeat

        sanitized = pattern.sub(_replace, sanitized)
    return sanitized, changed


def _sanitize_repetitive_output(value):
    """Recursively clean strings, lists or dicts returned by the LLM."""
    if isinstance(value, str):
        return _limit_repetitive_sequences(value)
    if isinstance(value, list):
        any_changed = False
        sanitized_list = []
        for item in value:
            sanitized_item, changed = _sanitize_repetitive_output(item)
            any_changed = any_changed or changed
            sanitized_list.append(sanitized_item)
        return sanitized_list, any_changed
    if isinstance(value, dict):
        any_changed = False
        sanitized_dict = {}
        for key, item in value.items():
            sanitized_item, changed = _sanitize_repetitive_output(item)
            any_changed = any_changed or changed
            sanitized_dict[key] = sanitized_item
    return sanitized_dict, any_changed
    return value, False
_EMOJI_PATTERN = re.compile('[\U0001F300-\U0001FAFF\U00002600-\U000026FF\U00002700-\U000027BF]')


def _sanitize_label_to_common_emoji(label: str):
    if not isinstance(label, str):
        return label
    if label in ALLOWED_EMOJIS:
        return label
    if _EMOJI_PATTERN.search(label):
        return COMMON_EMOJIS["æœªçŸ¥"]
    return label

def log_llm_call(prompt_key, final_prompt, raw_response, final_output):
    global LLM_LOG_BUFFER
    log_entry = (f"--- LLM Call @ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ---\n"
                 f"Prompt Key: {prompt_key}\nFinal Prompt:\n---\n{final_prompt}\n---\n"
                 f"Raw Response:\n---\n{raw_response}\n---\n"
                 f"Final Parsed Output:\n---\n{json.dumps(final_output, ensure_ascii=False, indent=2)}\n"
                 f"---------------------------------------------------\n")
    LLM_LOG_BUFFER.append(log_entry)
    if len(LLM_LOG_BUFFER) > MAX_LLM_LOG_LINES: LLM_LOG_BUFFER.pop(0)

def get_llm_log(): return "\n".join(LLM_LOG_BUFFER)

PROMPT_PATHS = {
    "generate_schedule": os.path.join(PROMPT_DIR, "ç”Ÿæˆæ—¥ç¨‹å®‰æ’æ—¶é—´è¡¨.txt"), "wake_up_hour": os.path.join(PROMPT_DIR, "èµ·åºŠæ—¶é—´.txt"),
    "pronunciatio": os.path.join(PROMPT_DIR, "è¡Œä¸ºè½¬ä¸ºå›¾æ ‡æ˜¾ç¤º.txt"), "double_chat": os.path.join(PROMPT_DIR, "èŠå¤©.txt"),
    "go_map": os.path.join(PROMPT_DIR, "è¡ŒåŠ¨éœ€è¦å»çš„åœ°æ–¹.txt"), "modify_schedule": os.path.join(PROMPT_DIR, "ç»†åŒ–æ¯æ—¥å®‰æ’æ—¶é—´è¡¨.txt"),
    "summarize_chat": os.path.join(PROMPT_DIR, "æ€»ç»“ç»å†äº¤è°ˆä¸ºè®°å¿†.txt"), "get_recovery_action": os.path.join(PROMPT_DIR, "è·å–æ¢å¤è¡Œå‹•.txt"),
    "summarize_disaster": os.path.join(PROMPT_DIR, "æ€»ç»“ç¾å®³ç»å†.txt"), "generate_initial_memory": os.path.join(PROMPT_DIR, "ç”Ÿæˆåˆå§‹è¨˜æ†¶.txt"),
    "inner_monologue": os.path.join(PROMPT_DIR, "ç”Ÿæˆå…§å¿ƒç¨ç™½.txt"), "generate_action_thought": os.path.join(PROMPT_DIR, "ç”Ÿæˆè¡Œå‹•æƒ³æ³•.txt"),
    "earthquake_step_action": os.path.join(PROMPT_DIR, "ç”Ÿæˆåœ°éœ‡ä¸­è¡Œå‹•.txt"), "generate_weekly_schedule": os.path.join(PROMPT_DIR, "ç”Ÿæˆä¸ƒæ—¥è¡Œäº‹æ›†.txt"),
}

ollama_agent = None
async def initialize_llm():
    global ollama_agent
    if ollama_agent is None:
        try:
            ollama_agent = OllamaAgent(MODEL_NAME, OLLAMA_URL)
            print(f"âœ… [SUCCESS] OllamaAgent (æµå¼) åˆå§‹åŒ–æˆåŠŸï¼Œæ¨¡å‹: {MODEL_NAME}")
            return True
        except Exception as e:
            print(f"âŒ [CRITICAL_ERROR] åˆå§‹åŒ– OllamaAgent (æµå¼) å¤±æ•—: {e}", file=sys.stderr)
            return False
    return True

async def close_llm_session():
    if ollama_agent: await ollama_agent.close_session(); print("OllamaAgent å°è©±å·²é—œé–‰ã€‚")

def _json_output_regex(text: str, default: any):
    if not isinstance(text, str): return default
    match = re.search(r"```json\s*(\{.*?\})\s*```", text, re.DOTALL)
    json_str = match.group(1) if match else None
    if not json_str:
        start, end = text.find('{'), text.rfind('}')
        if start != -1 and end != -1 and end > start: json_str = text[start:end+1]
        else:
            if isinstance(default, str): return text.strip()
            return default
    try:
        data = json.loads(json_str)
        return data.get("output", data)
    except json.JSONDecodeError:
        if isinstance(default, str): return text.strip()
        return default

async def _safe_llm_call(prompt_key: str, prompt_args: list, special_instruction: str, default_on_error: any):
    if not await initialize_llm(): return default_on_error
    prompt_template_path = PROMPT_PATHS.get(prompt_key)
    if not os.path.exists(prompt_template_path): return default_on_error
    raw_response, final_output, prompt = "N/A", default_on_error, ""
    try:
        processed_args = [str(arg) for arg in prompt_args]
        prompt = OllamaAgent.generate_prompt(processed_args, prompt_template_path)
        final_instruction = (
            f"{special_instruction} è«‹å‹™å¿…ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼ˆTraditional Chineseï¼‰å›ç­”ï¼Œ"
            "è«‹ç›´æ¥çµ¦å‡ºç²¾ç°¡çš„æœ€çµ‚è¼¸å‡ºï¼Œé¿å…å†—é•·çš„æ¨ç†æ­¥é©Ÿã€<think> æ¨™ç±¤æˆ–é‡è¤‡èªå¥ã€‚"
        )
        is_json_output = not isinstance(default_on_error, str)
        raw_response = await ollama_agent.ollama_stream_generate_response(prompt=prompt, special_instruction=final_instruction, expect_json=is_json_output, example_output=default_on_error)
        if raw_response is None: final_output = default_on_error
        else: final_output = _json_output_regex(raw_response, default_on_error) if is_json_output else raw_response
        if isinstance(final_output, str): converted_output = cc.convert(final_output)
        elif isinstance(final_output, dict): converted_output = {cc.convert(k): (cc.convert(v) if isinstance(v, str) else v) for k, v in final_output.items()}
        elif isinstance(final_output, list):
            def convert_list_items(item):
                if isinstance(item, str): return cc.convert(item)
                if isinstance(item, list): return [convert_list_items(i) for i in item]
                if isinstance(item, dict): return {cc.convert(k): (cc.convert(v) if isinstance(v, str) else v) for k, v in item.items()}
                return item
            converted_output = [convert_list_items(i) for i in final_output]
        else: converted_output = final_output
        converted_output, sanitized = _sanitize_repetitive_output(converted_output)
        if sanitized:
            print(f"âš ï¸ [LLM_OUTPUT_SANITIZED] åµæ¸¬åˆ° '{prompt_key}' å›å‚³å¤§é‡é‡è¤‡å…§å®¹ï¼Œå·²è‡ªå‹•è£åˆ‡ã€‚", file=sys.stderr)
        log_llm_call(prompt_key, prompt, str(raw_response), converted_output)
        return converted_output
    except Exception as e:
        print(f"âŒ [LLM_CALL_ERROR] åœ¨ LLM å‘¼å« '{prompt_key}' æœŸé–“ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}", file=sys.stderr)
        log_llm_call(prompt_key, prompt, str(e), default_on_error)
        return default_on_error

async def run_gpt_prompt_generate_initial_memory(name, mbti, persona_summary, home):
    default_memory = "è¨˜æ†¶ç”Ÿæˆå¤±æ•—ï¼Œè«‹æª¢æŸ¥LLMé€£ç·šã€‚"
    memory_result = await _safe_llm_call("generate_initial_memory", [name, mbti, persona_summary, home], 'åƒ…è¿”å›æè¿°ä»£ç†äººèƒŒæ™¯æ•…äº‹çš„ç´”æ–‡å­—å­—ä¸²ã€‚', default_memory)
    success = memory_result != default_memory and isinstance(memory_result, str)
    return str(memory_result), success
def _match_common_emoji(text: str):
    if not isinstance(text, str):
        return None
    for key, emoji in COMMON_EMOJIS.items():
        if key in text:
            return emoji
    return None


def _apply_common_emojis(schedule):
    if not isinstance(schedule, list):
        return schedule
    updated_schedule = []
    for entry in schedule:
        if isinstance(entry, list) and entry:
            label = entry[0]
            emoji = _match_common_emoji(str(label))
            new_entry = entry.copy()
            if emoji:
                new_entry[0] = emoji
            else:
                new_entry[0] = _sanitize_label_to_common_emoji(str(label))
            updated_schedule.append(new_entry)
            continue
        updated_schedule.append(_sanitize_label_to_common_emoji(entry) if isinstance(entry, str) else entry)
    return updated_schedule
async def run_gpt_prompt_generate_weekly_schedule(persona_summary):
    default_schedule = {day: COMMON_EMOJIS["ä¼‘æ¯"] for day in ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"]}
    schedule = await _safe_llm_call("generate_weekly_schedule", [persona_summary], 'è¿”å›ä¸€å€‹åŒ…å«ä¸ƒå¤©ï¼ˆMonday-Sundayï¼‰éµçš„ JSON ç‰©ä»¶ã€‚', default_schedule)
    if isinstance(schedule, dict):
        schedule = {
            day: (_match_common_emoji(str(activity)) or _sanitize_label_to_common_emoji(str(activity)))
            for day, activity in schedule.items()
        }
    success = schedule != default_schedule and isinstance(schedule, dict) and len(schedule) == 7
    return schedule, success

async def run_gpt_prompt_generate_hourly_schedule(persona, now_time, today_goal):
    default_on_error = [[COMMON_EMOJIS["ä¼‘æ¯"], 1440]]
    schedule = await _safe_llm_call("generate_schedule", [persona, now_time, today_goal], 'è¿”å›ä¸€å€‹åˆ—è¡¨ï¼Œå…¶ä¸­æ¯å€‹å­åˆ—è¡¨åŒ…å«[æ´»å‹•åç¨±, æŒçºŒåˆ†é˜æ•¸]ã€‚', default_on_error)
    return _apply_common_emojis(schedule)
async def run_gpt_prompt_wake_up_hour(persona, now_time, hourly_schedule):
    schedule_str = json.dumps(hourly_schedule, ensure_ascii=False)
    default_on_error = f"{random.randint(6, 8):02d}-{random.choice(['00', '15', '30'])}"
    raw_llm_output = await _safe_llm_call("wake_up_hour", [persona, now_time, schedule_str], 'è¿”å› "HH:MM" æˆ– "HH-MM" æ ¼å¼çš„æ™‚é–“å­—ä¸²ã€‚', default_on_error)
    match = re.search(r'\b([0-1][0-9]|2[0-3])[:|-]([0-5][0-9])\b', str(raw_llm_output))
    return f"{match.group(1)}-{match.group(2)}" if match else default_on_error

async def run_gpt_prompt_pronunciatio(action_dec):
    action_str = str(action_dec)
    emoji = _match_common_emoji(action_str)
    if emoji:
        return emoji
    result = await _safe_llm_call("pronunciatio", [action_str], 'åªè¿”å›ä¸€å€‹æœ€é©åˆçš„ emoji åœ–æ¨™å­—ä¸²ã€‚', COMMON_EMOJIS["æœªçŸ¥"])
    return _sanitize_label_to_common_emoji(result)

async def generate_action_thought(persona_summary, current_place, new_action):
    return await _safe_llm_call("generate_action_thought", [persona_summary, current_place, new_action], 'è¿”å›ä¸€å¥ç´„20å­—çš„ç°¡çŸ­å…§å¿ƒæƒ³æ³•å­—ä¸²ã€‚', "")

async def run_gpt_prompt_earthquake_step_action(persona_summary, health, mental_state, current_place, intensity, disaster_log):
    default_on_error = {"action": "ä¿æŒè­¦æƒ•", "thought": "(ææ‡¼ä¸­...)"}
    log_context = "\n".join(disaster_log)
    parsed_output = await _safe_llm_call("earthquake_step_action", [persona_summary, health, mental_state, current_place, intensity, log_context], 'è¼¸å‡ºåŒ…å« "action" å’Œ "thought" éµçš„ JSON ç‰©ä»¶ã€‚', default_on_error)
    return parsed_output.get("action", default_on_error["action"]), parsed_output.get("thought", default_on_error["thought"])

async def double_agents_chat(chat_context):
    default_on_error = {"thought": "è§£æéŒ¯èª¤ã€‚", "dialogue": []}
    prompt_args = [
        chat_context['location'], chat_context['agent1']['name'], chat_context['agent1']['mbti'], chat_context['agent1']['persona'], str(chat_context['agent1']['memory'])[-300:],
        chat_context['agent2']['name'], chat_context['agent2']['mbti'], chat_context['agent2']['persona'], str(chat_context['agent2']['memory'])[-300:],
        chat_context['now_time'], chat_context['agent1']['action'], chat_context['agent2']['action'],
        chat_context.get('eq_ctx') or "ç›®å‰ä¸€åˆ‡æ­£å¸¸ã€‚", json.dumps(chat_context['history'], ensure_ascii=False) ]
    parsed_output = await _safe_llm_call(
        "double_chat",
        prompt_args,
        'è¼¸å‡ºä¸€å€‹åŒ…å« "thought" å’Œ "dialogue" éµçš„ JSON ç‰©ä»¶ï¼Œdialogue è«‹é™åˆ¶ 2~4 å¥ï¼Œæ¯å¥ä¸è¶…é 20 å­—ï¼Œé¿å…é‡è¤‡æˆ–è´…è©ã€‚',
        default_on_error
    )
    return parsed_output.get("thought", default_on_error["thought"]), parsed_output.get("dialogue", default_on_error["dialogue"])

async def generate_inner_monologue(agent_context):
    default_on_error = {"thought": "è§£æéŒ¯èª¤ã€‚", "monologue": "ï¼ˆæ­£åœ¨æ€è€ƒ...ï¼‰"}
    prompt_args = [
        agent_context['name'], agent_context['mbti'], agent_context['persona'], agent_context['location'],
        agent_context['action'], agent_context['now_time'], str(agent_context['memory'])[-300:],
        agent_context.get('eq_ctx') or "ç›®å‰ä¸€åˆ‡æ­£å¸¸ã€‚" ]
    parsed_output = await _safe_llm_call(
        "inner_monologue",
        prompt_args,
        'è¼¸å‡ºä¸€å€‹åŒ…å« "thought" å’Œ "monologue" éµçš„ JSON ç‰©ä»¶ï¼Œmonologue å…§å®¹è«‹æ§åˆ¶åœ¨ 25 å­—ä»¥å…§ä¸¦é¿å…é‡è¤‡èªå¥ã€‚',
        default_on_error
    )
    return parsed_output.get("thought", default_on_error["thought"]), parsed_output.get("monologue", default_on_error["monologue"])
    
async def run_gpt_prompt_summarize_disaster(agent_name, mbti, health, experience_log):
    log_str = "\n".join([str(entry) for entry in experience_log]) or "(æ²’æœ‰å…·é«”äº‹ä»¶è¨˜éŒ„)"
    return await _safe_llm_call("summarize_disaster", [agent_name, mbti, health, log_str], 'è¿”å›ç°¡çŸ­çš„ç½å¾Œè¨˜æ†¶ç¸½çµå­—ä¸²ã€‚', "ç¶“æ­·äº†ä¸€å ´åœ°éœ‡ï¼Œç¾åœ¨å®‰å…¨ã€‚")

async def run_gpt_prompt_get_recovery_action(persona, mental_state, curr_place):
    return await _safe_llm_call("get_recovery_action", [persona, mental_state, curr_place], 'è¿”å›å»ºè­°çš„æ¢å¾©è¡Œå‹•çŸ­èªå­—ä¸²ã€‚', "åŸåœ°ä¼‘æ¯")


async def go_map_async(agent_name, home, curr_place, can_go_list, curr_task):
    can_go_str = ", ".join(can_go_list)
    destination = await _safe_llm_call(
        "go_map",
        [agent_name, home, curr_place, can_go_str, curr_task],
        'åªè¿”å›ç›®æ¨™åœ°é»åç¨±çš„ç´”æ–‡å­—ã€‚',
        home,
    )
    if isinstance(destination, str) and destination in can_go_list:
        return destination
    return home


async def modify_schedule_async(old_sched, now_time, memory, wake_time, role):
    old_sched = old_sched or []
    try:
        old_sched_str = json.dumps(old_sched, ensure_ascii=False)
    except TypeError:
        old_sched_str = str(old_sched)
    parsed_output = await _safe_llm_call(
        "modify_schedule",
        [old_sched_str, now_time, memory, wake_time, role],
        'è¿”å›èª¿æ•´å¾Œçš„æ—¥ç¨‹åˆ—è¡¨ï¼Œæ¯å€‹å…ƒç´ ç‚º[æ´»å‹•åç¨±, æŒçºŒåˆ†é˜æ•¸]ã€‚',
        old_sched,
    )
    if isinstance(parsed_output, list) and all(isinstance(item, (list, tuple)) and len(item) >= 2 for item in parsed_output):
        return _apply_common_emojis([list(item[:2]) for item in parsed_output])
    return old_sched


async def summarize_async(memory, now_time, name):
    default_summary = "ä»Šå¤©æ²’æœ‰ç‰¹åˆ¥çš„å°è©±ã€‚"
    summary = await _safe_llm_call(
        "summarize_chat",
        [memory, now_time, name],
        'åªè¿”å›ç°¡çŸ­çš„èŠå¤©ç¸½çµã€‚',
        default_summary,
    )
    return summary if isinstance(summary, str) else default_summary


def _run_async(coro):
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        loop = None
    if loop and loop.is_running():
        return asyncio.ensure_future(coro)
    return asyncio.run(coro)


def go_map(agent_name, home, curr_place, can_go_list, curr_task):
    result = _run_async(go_map_async(agent_name, home, curr_place, can_go_list, curr_task))
    if asyncio.isfuture(result):
        raise RuntimeError("go_map() was called inside an active event loop. Please await go_map_async() instead.")
    return result


def modify_schedule(old_sched, now_time, memory, wake_time, role):
    result = _run_async(modify_schedule_async(old_sched, now_time, memory, wake_time, role))
    if asyncio.isfuture(result):
        raise RuntimeError("modify_schedule() was called inside an active event loop. Please await modify_schedule_async() instead.")
    return result


def summarize(memory, now_time, name):
    result = _run_async(summarize_async(memory, now_time, name))
    if asyncio.isfuture(result):
        raise RuntimeError("summarize() was called inside an active event loop. Please await summarize_async() instead.")
    return result
