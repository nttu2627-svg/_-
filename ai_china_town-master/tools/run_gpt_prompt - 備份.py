import json
import re
import os
import traceback
from .ollama_agent import OllamaAgent # ä½¿ç”¨ç›¸å°å°å…¥
from datetime import datetime

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ å…¨å±€é…ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
OLLAMA_URL = "http://127.0.0.1:11434"
MODEL_NAME = "deepseek-32b" # è«‹ç¢ºä¿æ­¤æ¨¡å‹å·²é€šé 'ollama pull deepseek-32b' ä¸‹è¼‰

PROMPT_DIR = os.path.join(os.path.dirname(__file__), 'prompt_template')
os.makedirs(PROMPT_DIR, exist_ok=True)

PROMPT_PATHS = {
    "generate_schedule": os.path.join(PROMPT_DIR, "ç”Ÿæˆæ—¥ç¨‹å®‰æ’æ—¶é—´è¡¨.txt"),
    "wake_up_hour": os.path.join(PROMPT_DIR, "èµ·åºŠæ—¶é—´.txt"),
    "pronunciatio": os.path.join(PROMPT_DIR, "è¡Œä¸ºè½¬ä¸ºå›¾æ ‡æ˜¾ç¤º.txt"),
    "double_chat": os.path.join(PROMPT_DIR, "èŠå¤©.txt"),
    "go_map": os.path.join(PROMPT_DIR, "è¡ŒåŠ¨éœ€è¦å»çš„åœ°æ–¹.txt"),
    "modify_schedule": os.path.join(PROMPT_DIR, "ç»†åŒ–æ¯æ—¥å®‰æ’æ—¶é—´è¡¨.txt"),
    "summarize_chat": os.path.join(PROMPT_DIR, "æ€»ç»“ç»å†äº¤è°ˆä¸ºè®°å¿†.txt"),
    "get_recovery_action": os.path.join(PROMPT_DIR, "è·å–æ¢å¤è¡ŒåŠ¨.txt"),
    "summarize_disaster": os.path.join(PROMPT_DIR, "æ€»ç»“ç¾å®³ç»å†.txt"),
    "inner_monologue": os.path.join(PROMPT_DIR, "ç”Ÿæˆå…§å¿ƒç¨ç™½.txt"),
    "generate_initial_memory": os.path.join(PROMPT_DIR, "ç”Ÿæˆåˆå§‹è¨˜æ†¶.txt"),
}

DEFAULT_PROMPTS = {
    "generate_schedule": "åŸºæ–¼è§’è‰² !<INPUT 0>! çš„èƒŒæ™¯å’Œç•¶å‰æ™‚é–“ !<INPUT 1>!ï¼Œç‚ºä»–/å¥¹ç”Ÿæˆä¸€ä»½åŒ…å«[æ´»å‹•åç¨±, é è¨ˆæŒçºŒåˆ†é˜æ•¸]çš„æ—¥ç¨‹åˆ—è¡¨ã€‚è«‹è€ƒæ…®è§’è‰²çš„æ€§æ ¼ç‰¹é»å’Œå¯èƒ½çš„æ—¥å¸¸éœ€æ±‚ã€‚",
    "wake_up_hour": "åŸºæ–¼è§’è‰² !<INPUT 0>! çš„èƒŒæ™¯ï¼Œä»Šå¤©çš„æ—¥æœŸ !<INPUT 1>! å’Œå¤§è‡´æ—¥ç¨‹ !<INPUT 2>!ï¼Œæ±ºå®šä¸€å€‹åˆç†çš„èµ·åºŠæ™‚é–“ã€‚è¼¸å‡ºæ ¼å¼ç‚º HH-MMã€‚",
    "pronunciatio": "ç‚ºä»¥ä¸‹è¡Œå‹• '!<INPUT 0>!' ç”Ÿæˆä¸€å€‹ç°¡æ½”ä¸”å…·ä»£è¡¨æ€§çš„è¡¨æƒ…ç¬¦è™Ÿã€‚",
    "double_chat": """
    æ¨¡æ“¬ä¸€å ´ç™¼ç”Ÿåœ¨ !<INPUT 0>! (åœ°é») çš„å°è©±ï¼Œåƒèˆ‡è€…æ˜¯ï¼š
    - !<INPUT 1>! (è§’è‰²1)
    - !<INPUT 2>! (è§’è‰²2)

    ç•¶å‰æƒ…å¢ƒï¼š
    !<INPUT 3>!

    è§’è‰² !<INPUT 1>! çš„è¿‘æœŸå°è©±æ‘˜è¦ï¼š
    !<INPUT 4>!

    è§’è‰² !<INPUT 2>! çš„è¿‘æœŸå°è©±æ‘˜è¦ï¼š
    !<INPUT 5>!

    æ¨¡æ“¬æ—¥æœŸï¼š!<INPUT 6>!
    è¿‘æœŸç‰¹æ®Šäº‹ä»¶èƒŒæ™¯ï¼š!<INPUT 7>!

    è«‹æ ¹æ“šä»–å€‘çš„æ€§æ ¼ç‰¹é»ã€ç•¶å‰æƒ…å¢ƒä»¥åŠç‰¹æ®Šäº‹ä»¶èƒŒæ™¯ï¼Œç”Ÿæˆä¸€æ®µè‡ªç„¶ä¸”ç¬¦åˆé‚è¼¯çš„å°è©±ã€‚
    å°è©±æ‡‰åŒ…å«å¤šè¼ªäº¤æµï¼Œç¸½é•·åº¦ç´„50åˆ°150å­—ã€‚
    å¦‚æœè¿‘æœŸäº‹ä»¶èƒŒæ™¯æåŠäº†åœ°éœ‡ï¼Œè«‹è®“ä»–å€‘çš„å°è©±é©ç•¶åœ°åœç¹åœ°éœ‡çš„å½±éŸ¿ã€æ„Ÿå—æˆ–å¾ŒçºŒè¨ˆåŠƒå±•é–‹ã€‚
    ç¢ºä¿å°è©±é¢¨æ ¼ç¬¦åˆå„è‡ªçš„MBTIæ€§æ ¼ã€‚
    """,
    "go_map": "è§’è‰² !<INPUT 0>! (å®¶åœ¨ !<INPUT 1>!) ç•¶å‰åœ¨ !<INPUT 2>!ï¼Œæƒ³è¦åŸ·è¡Œ '!<INPUT 4>!'ã€‚é™„è¿‘çš„å¯é¸åœ°é»æœ‰ï¼š!<INPUT 3>!ã€‚ä»–/å¥¹æœ€æ‡‰è©²å»å“ªå€‹åœ°é»ï¼Ÿè«‹åªå›ç­”åœ°é»åç¨±ã€‚",
    "modify_schedule": "æ ¹æ“šè§’è‰² !<INPUT 4>! çš„èˆŠæ—¥ç¨‹ !<INPUT 0>!ï¼Œç•¶å‰æ™‚é–“ !<INPUT 1>!ï¼Œè¿‘æœŸè¨˜æ†¶ !<INPUT 2>!ï¼Œå’Œèµ·åºŠæ™‚é–“ !<INPUT 3>!ï¼Œé©ç•¶èª¿æ•´ä¸¦å„ªåŒ–ä»–/å¥¹çš„æ—¥ç¨‹å®‰æ’ã€‚è«‹è€ƒæ…®è¨˜æ†¶ä¸­å¯èƒ½æåˆ°çš„çªç™¼äº‹ä»¶æˆ–æ–°è¨ˆåŠƒã€‚",
    "summarize_chat": "è«‹å°‡ !<INPUT 2>! (è§’è‰²å) åœ¨ !<INPUT 1>! (æ—¥æœŸ) çš„é€™æ®µèŠå¤©è¨˜éŒ„ '!<INPUT 0>!' ç¸½çµç‚ºä¸€æ®µç°¡çŸ­ï¼ˆä¸è¶…é50å­—ï¼‰çš„è¨˜æ†¶ã€‚",
    "get_recovery_action": "è§’è‰²èƒŒæ™¯: !<INPUT 0>!\nç•¶å‰ç²¾ç¥ç‹€æ…‹: !<INPUT 1>!\næ‰€åœ¨ä½ç½®: !<INPUT 2>!\nå‰›å‰›ç¶“æ­·äº†ä¸€å ´åœ°éœ‡ï¼Œç¾åœ¨æ˜¯ç½å¾Œæ¢å¾©æœŸã€‚è«‹å»ºè­°ä¸€å€‹æœ€å„ªå…ˆçš„ã€ç°¡çŸ­çš„æ¢å¾©è¡Œå‹•ã€‚",
    "summarize_disaster": "è§’è‰² !<INPUT 0>! (MBTI: !<INPUT 1>!) å‰›å‰›ç¶“æ­·äº†ä¸€å ´ç½é›£ã€‚ä»–/å¥¹ç›®å‰çš„å¥åº·ç‹€æ³æ˜¯ !<INPUT 2>!/100ã€‚ä»¥ä¸‹æ˜¯ç½é›£æœŸé–“çš„é—œéµç¶“æ­·è¨˜éŒ„ï¼š\n!<INPUT 3>!\nè«‹æ ¹æ“šé€™äº›ä¿¡æ¯ï¼Œç‚ºè§’è‰²ç”Ÿæˆä¸€æ®µç¬¬ä¸€äººç¨±çš„ã€ç°¡çŸ­ï¼ˆç´„50-100å­—ï¼‰çš„ç½å¾Œè¨˜æ†¶ç¸½çµï¼Œåæ˜ ä»–/å¥¹çš„æ„Ÿå—å’Œç‹€æ…‹ã€‚",
}

for key, path in PROMPT_PATHS.items():
    if not os.path.exists(path):
        print(f"âš ï¸ [WARN] æç¤ºè©æª”æ¡ˆä¸å­˜åœ¨: {path}ã€‚æ­£åœ¨å‰µå»ºé è¨­æª”æ¡ˆã€‚")
        try:
            with open(path, 'w', encoding='utf-8') as f:
                f.write(DEFAULT_PROMPTS.get(key, f"Default prompt for {key}"))
        except Exception as e:
            print(f"âŒ [ERROR] å‰µå»ºé è¨­æç¤ºè©æª”æ¡ˆ {path} å¤±æ•—: {e}")

try:
    ollama_agent = OllamaAgent(MODEL_NAME, OLLAMA_URL, "agent_simulator_v3")
    print(f"âœ… [SUCCESS] OllamaAgent åˆå§‹åŒ–æˆåŠŸï¼Œæ¨¡å‹: {MODEL_NAME}, URL: {OLLAMA_URL}")
    LLM_INITIALIZED_SUCCESSFULLY = True
except Exception as e:
    print(f"âŒ [CRITICAL_ERROR] åˆå§‹åŒ– OllamaAgent å¤±æ•—ã€‚è«‹ç¢ºèª Ollama æœå‹™æ­£åœ¨é‹è¡Œä¸”æ¨¡å‹ '{MODEL_NAME}' å·²ä¸‹è¼‰ã€‚")
    print(f"    è©³ç´°éŒ¯èª¤: {e}")
    ollama_agent = None
    LLM_INITIALIZED_SUCCESSFULLY = False

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ é è¨­è¿”å›å€¼ (Fallback Values) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_DEF_EMOJI = "â“"
_DEF_WAKE  = "07-00"
_DEF_PLAN  = [["ä¼‘æ¯", 180]]
_DEF_RECOVERY_ACTION = "åŸåœ°ä¼‘æ¯"
_DEF_DISASTER_SUMMARY = "ç¶“æ­·äº†ä¸€å ´åœ°éœ‡ï¼Œç¾åœ¨å®‰å…¨ã€‚"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ æ ¸å¿ƒå‡½æ•¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _json_output_regex(text: str, default: any):
    if not text or not isinstance(text, str):
        return default
    
    match = re.search(r"```json\s*(\{.*?\})\s*```", text, re.DOTALL)
    if match:
        text = match.group(1)
    
    try:
        data = json.loads(text)
        return data.get("output", default)
    except json.JSONDecodeError:
        match = re.search(r'\{\s*"output"\s*:\s*(?P<value>.*?)\s*\}', text, re.DOTALL)
        if match:
            value_str = match.group("value").strip()
            if value_str.endswith(','): value_str = value_str[:-1]
            try:
                return json.loads(f'{{"output": {value_str}}}')['output']
            except json.JSONDecodeError:
                if value_str.startswith('"') and value_str.endswith('"'): return value_str[1:-1]
                return value_str
        return default

def _safe_llm_call(prompt_key: str, prompt_args: list, example_output: any, special_instruction: str, repeat: int, fail_safe_json: any, default_on_error: any):
    """
    å¸¶æœ‰è©³ç´°æ—¥èªŒå’ŒéŒ¯èª¤è™•ç†çš„ LLM èª¿ç”¨å°è£å™¨
    """
    print(f"â¡ï¸  [LLM_CALL] å˜—è©¦å‘¼å« '{prompt_key}'...")

    if not LLM_INITIALIZED_SUCCESSFULLY or ollama_agent is None:
        print(f"    - âŒ [LLM_CALL_FAIL] å›  OllamaAgent æœªæˆåŠŸåˆå§‹åŒ–ï¼Œå‘¼å« '{prompt_key}' ä¸­æ–·ï¼Œè¿”å›é è¨­å€¼ã€‚")
        return default_on_error

    prompt_template_path = PROMPT_PATHS.get(prompt_key)
    if not prompt_template_path or not os.path.exists(prompt_template_path):
        print(f"    - âŒ [LLM_CALL_FAIL] æ‰¾ä¸åˆ°æç¤ºè©æ¨¡æ¿æª”æ¡ˆ {prompt_key} ({prompt_template_path})ã€‚è¿”å›é è¨­å€¼ã€‚")
        return default_on_error

    try:
        processed_prompt_args = [str(arg) if not isinstance(arg, str) else arg for arg in prompt_args]
        prompt = OllamaAgent.generate_prompt(processed_prompt_args, prompt_template_path)
        
        final_instruction = f"{special_instruction} è«‹å‹™å¿…ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼ˆTraditional Chineseï¼‰å›ç­”ã€‚"

        output_str = ollama_agent.ollama_safe_generate_response(
            prompt=prompt,
            example_output=example_output,
            special_instruction=final_instruction,
            repeat=repeat,
            func_validate=lambda x: isinstance(x, str) and len(x) > 5,
            func_clean_up=lambda x: x.strip(),
            fail_safe=fail_safe_json,
        )

        print(f"    - ğŸ“„ [LLM_RAW_OUTPUT] å¾ '{prompt_key}' æ”¶åˆ°åŸå§‹è¼¸å‡º: {str(output_str)[:250]}...")
        
        parsed_output = _json_output_regex(output_str, default_on_error)
        
        print(f"    - âœ¨ [LLM_PARSED_OUTPUT] è§£æå¾Œè¼¸å‡º: {parsed_output}")
        print(f"âœ… [LLM_CALL_SUCCESS] æˆåŠŸå®Œæˆ '{prompt_key}' å‘¼å«ã€‚")
        
        return parsed_output

    except Exception as e:
        print(f"âŒ [LLM_CALL_ERROR] åœ¨ LLM å‘¼å« '{prompt_key}' æœŸé–“ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}")
        print(traceback.format_exc())
        print(f"    - âš ï¸ è¿”å›é è¨­å€¼: {default_on_error}")
        return default_on_error

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ å°å‡ºçš„ LLM å‡½æ•¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def run_gpt_prompt_generate_hourly_schedule(persona, now_time):
    example = [["åƒæ—©é¤", 30], ["ä¸Šèª²", 120]]
    parsed_output = _safe_llm_call(
        prompt_key="generate_schedule", prompt_args=[persona, now_time],
        example_output=example,
        special_instruction='è«‹ä»¥åŒ…å«åç‚º "output" çš„å–®ä¸€éµçš„ JSON ç‰©ä»¶æ ¼å¼è¿”å›ã€‚è©²éµçš„å€¼æ‡‰ç‚ºä¸€å€‹åˆ—è¡¨ï¼Œå…¶ä¸­æ¯å€‹å­åˆ—è¡¨åŒ…å«[æ´»å‹•åç¨±, é è¨ˆæŒçºŒåˆ†é˜æ•¸]ã€‚',
        repeat=3, fail_safe_json={"output": _DEF_PLAN}, default_on_error=_DEF_PLAN
    )
    if isinstance(parsed_output, list) and all(isinstance(i, (list, tuple)) and len(i) == 2 for i in parsed_output):
        return parsed_output
    print(f"âš ï¸ [VALIDATION_FAIL] 'generate_schedule' çš„è¼¸å‡ºæ ¼å¼ä¸æ­£ç¢ºï¼Œè¿”å›é è¨­æ—¥ç¨‹ã€‚æ”¶åˆ°çš„å…§å®¹: {parsed_output}")
    return _DEF_PLAN

def run_gpt_prompt_wake_up_hour(persona, now_time, hourly_schedule):
    schedule_str = json.dumps(hourly_schedule, ensure_ascii=False)
    wake_time = _safe_llm_call(
        prompt_key="wake_up_hour", prompt_args=[persona, now_time, schedule_str],
        example_output="07-15",
        special_instruction='è«‹ä»¥åŒ…å«åç‚º "output" çš„å–®ä¸€éµçš„ JSON ç‰©ä»¶æ ¼å¼è¿”å›ã€‚è©²éµçš„å€¼æ‡‰ç‚º "HH-MM" æ ¼å¼çš„èµ·åºŠæ™‚é–“å­—ç¬¦ä¸²ã€‚',
        repeat=3, fail_safe_json={"output": _DEF_WAKE}, default_on_error=_DEF_WAKE
    )
    if isinstance(wake_time, str) and re.match(r"^\d{2}-\d{2}$", wake_time):
         return wake_time
    print(f"âš ï¸ [VALIDATION_FAIL] 'wake_up_hour' çš„è¼¸å‡ºæ ¼å¼ä¸æ­£ç¢ºï¼Œè¿”å›é è¨­æ™‚é–“ã€‚æ”¶åˆ°çš„å…§å®¹: {wake_time}")
    return _DEF_WAKE

def run_gpt_prompt_pronunciatio(action_dec):
    common_emojis = {
        "ç¡è¦º": "ğŸ˜´", "ä¼‘æ¯": "ğŸ›‹ï¸", "åƒé£¯": "ğŸ•", "æ—©é¤": "ğŸ³", "åˆé¤": "ğŸ”", "æ™šé¤": "ğŸœ", "ä¸Šèª²": "ğŸ“š",
        "å­¸ç¿’": "âœï¸", "å·¥ä½œ": "ğŸ’¼", "é–‹æœƒ": "ğŸ—£ï¸", "ç§»å‹•": "ğŸš¶", "è·‘æ­¥": "ğŸƒ", "å¥èº«": "ğŸ‹ï¸", "èµ°è·¯": "ğŸš¶â€â™€ï¸",
        "çœ‹é›»å½±": "ğŸ¬", "ç©éŠæˆ²": "ğŸ®", "çœ‹æ›¸": "ğŸ“–", "è½éŸ³æ¨‚": "ğŸ§", "èŠå¤©": "ğŸ’¬", "æ‰“é›»è©±": "ğŸ“",
        "è³¼ç‰©": "ğŸ›’", "åšé£¯": "ğŸ§‘â€ğŸ³", "æ´—æ¾¡": "ğŸ›€", "é†’ä¾†": "â˜€ï¸", "æ¢å¾©ä¸­": "ğŸ©¹", "æª¢æŸ¥": "ğŸ‘€",
        "æœç´¢": "ğŸ”", "Unconscious": "ğŸ˜µ", "lead": "ğŸ§‘â€ğŸš’", "panic": "ğŸ˜±", "flee": "ğŸ’¨", "freeze": "ğŸ¥¶",
        "calm": "ğŸ§˜", "assist_others": "ğŸ¤", "injured_flee": "ğŸ¤•", "recovering": "ğŸ˜Œ",
        "Initializing": "â³", "æ™‚é–“éŒ¯èª¤": "â“", "è©•ä¼°æå‚·": "ğŸ§", "å°‹æ±‚å”åŠ©": "ğŸ†˜", "åˆå§‹åŒ–ä¸­": "â³"
    }
    for key, emoji in common_emojis.items():
        if key in action_dec:
            return emoji
    return _DEF_EMOJI

def double_agents_chat(
    location,
    agent1, agent2,
    chat_history: list,
    eq_ctx=None
):
    """
    ç”Ÿæˆå…©å€‹ Agent ä¹‹é–“çš„å¤šè¼ªã€æœ‰ä¸Šä¸‹æ–‡çš„å°è©±ã€‚
    agent1 å’Œ agent2 æ˜¯ TownAgent ç‰©ä»¶ã€‚
    """
    earthquake_context = eq_ctx if eq_ctx else "ç›®å‰ä¸€åˆ‡æ­£å¸¸ã€‚"
    
    # æº–å‚™ Prompt æ‰€éœ€çš„åƒæ•¸
    a1_name = agent1.name
    a2_name = agent2.name
    a1_mbti = agent1.MBTI
    a2_mbti = agent2.MBTI
    a1_memory = agent1.memory[-500:] # å–æœ€è¿‘500å­—çš„è¨˜æ†¶
    a2_memory = agent2.memory[-500:]
    now_time = datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M') # å‡è¨­æˆ‘å€‘éœ€è¦ç•¶å‰æ™‚é–“
    a1_original_action = agent1.interrupted_action or agent1.last_action
    a2_original_action = agent2.interrupted_action or agent2.last_action
    
    # å°‡å°è©±æ­·å²æ ¼å¼åŒ–ç‚ºå­—ä¸²
    chat_history_str = "\n".join([f"{name}: {dialog}" for name, dialog in chat_history])

    example = {
        "thought": "ä¸¹å°¼çœ‹åˆ°è˜‡å…‹å¾ˆé©šè¨ï¼Œå¯èƒ½æœƒå•ä»–ç‚ºä»€éº¼åœ¨é€™è£¡ã€‚è˜‡å…‹æ¯”è¼ƒå…§å‘ï¼Œå¯èƒ½æœƒç°¡å–®å›ç­”ã€‚",
        "dialogue": [["ä¸¹å°¼","è˜‡å…‹ï¼ŸçœŸå·§ï¼Œä½ æ€éº¼æœƒåœ¨é€™ï¼Ÿ"],["è˜‡å…‹","...ä¾†è²·é»æ±è¥¿ã€‚"]]
    }
    fail_safe = {"output": {"thought": "LLM æ€è€ƒå¤±æ•—ã€‚", "dialogue": []}}

    new_prompt_args = [
        location,                       # 0
        a1_name,                        # 1
        a1_mbti,                        # 2
        agent1.personality_desc,        # 3 (ä½¿ç”¨æ›´è©³ç´°çš„æ€§æ ¼æè¿°)
        a1_memory,                      # 4
        a2_name,                        # 5
        a2_mbti,                        # 6
        agent2.personality_desc,        # 7
        a2_memory,                      # 8
        now_time,                       # 9
        a1_original_action,             # 10
        a2_original_action,             # 11
        earthquake_context,             # 12
        chat_history_str                # 13
    ]

    parsed_output = _safe_llm_call(
        prompt_key="double_chat", 
        prompt_args=new_prompt_args,
        example_output=example,
        special_instruction='è«‹ä»¥åŒ…å«åç‚º "output" çš„å–®ä¸€éµçš„ JSON ç‰©ä»¶æ ¼å¼è¿”å›ã€‚è©²éµçš„å€¼æ‡‰ç‚ºä¸€å€‹åˆ—è¡¨ï¼Œå…¶ä¸­æ¯å€‹å­åˆ—è¡¨åŒ…å«["èªªè©±è€…åç¨±", "è©±èªå…§å®¹"]ã€‚',
        repeat=3, 
        fail_safe_json=fail_safe, 
        default_on_error=[]
    )
    thought = parsed_output.get("thought", "æ€è€ƒéç¨‹æå–å¤±æ•—ã€‚")
    dialogue = parsed_output.get("dialogue", [])
    if isinstance(parsed_output, list) and all(isinstance(i, list) and len(i) == 2 for i in parsed_output):
        return parsed_output
    
    print(f"âš ï¸ [VALIDATION_FAIL] 'double_agents_chat' çš„è¼¸å‡ºæ ¼å¼ä¸æ­£ç¢ºï¼Œè¿”å›ç©ºå°è©±ã€‚æ”¶åˆ°çš„å…§å®¹: {parsed_output}")
    return []
def go_map(agent_name, home, curr_place, can_go_list, curr_task):
    can_go_str = ", ".join(can_go_list)
    destination = _safe_llm_call(
        prompt_key="go_map", prompt_args=[agent_name, home, curr_place, can_go_str, curr_task],
        example_output="æµ·é‚Š",
        special_instruction='è«‹ä»¥åŒ…å«åç‚º "output" çš„å–®ä¸€éµçš„ JSON ç‰©ä»¶æ ¼å¼è¿”å›ã€‚è©²éµçš„å€¼æ‡‰ç‚ºç›®æ¨™åœ°é»åç¨±å­—ç¬¦ä¸²ã€‚',
        repeat=3, fail_safe_json={"output": home}, default_on_error=home
    )
    if isinstance(destination, str) and destination in can_go_list:
        return destination
    print(f"âš ï¸ [VALIDATION_FAIL] 'go_map' çš„è¼¸å‡ºåœ°é»ç„¡æ•ˆ '{destination}'ï¼Œè¿”å›å®¶ä¸­ã€‚")
    return home

def modify_schedule(old_sched, now_time, memory, wake_time, role):
    old_sched_str = json.dumps(old_sched, ensure_ascii=False)
    parsed_output = _safe_llm_call(
        prompt_key="modify_schedule", prompt_args=[old_sched_str, now_time, memory, wake_time, role],
        example_output=[["åƒæ—©é¤", 45], ["é–±è®€", 60]],
        special_instruction='è«‹ä»¥åŒ…å«åç‚º "output" çš„å–®ä¸€éµçš„ JSON ç‰©ä»¶æ ¼å¼è¿”å›ã€‚è©²éµçš„å€¼æ‡‰ç‚ºèª¿æ•´å¾Œçš„æ—¥ç¨‹åˆ—è¡¨ï¼Œæ ¼å¼åŒè¼¸å…¥ã€‚',
        repeat=4, fail_safe_json={"output": old_sched}, default_on_error=old_sched
    )
    if isinstance(parsed_output, list) and all(isinstance(i, (list, tuple)) and len(i) == 2 for i in parsed_output):
        return parsed_output
    print(f"âš ï¸ [VALIDATION_FAIL] 'modify_schedule' çš„è¼¸å‡ºæ ¼å¼ä¸æ­£ç¢ºï¼Œè¿”å›èˆŠæ—¥ç¨‹ã€‚æ”¶åˆ°çš„å…§å®¹: {parsed_output}")
    return old_sched

def summarize(memory, now_time, name):
    summary = _safe_llm_call(
        prompt_key="summarize_chat", prompt_args=[memory, now_time, name],
        example_output="ä»Šå¤©å’Œå°èŠ³èŠäº†é€±æœ«å»å…¬åœ’çš„äº‹ï¼Œæ„Ÿè¦ºå¾ˆæœŸå¾…ã€‚",
        special_instruction='è«‹ä»¥åŒ…å«åç‚º "output" çš„å–®ä¸€éµçš„ JSON ç‰©ä»¶æ ¼å¼è¿”å›ã€‚è©²éµçš„å€¼æ‡‰ç‚ºç°¡çŸ­çš„èŠå¤©ç¸½çµå­—ç¬¦ä¸²ã€‚',
        repeat=3, fail_safe_json={"output":"ä»Šå¤©æ²’æœ‰ç‰¹åˆ¥çš„å°è©±ã€‚"}, default_on_error="æœªèƒ½ç¸½çµå°è©±ã€‚"
    )
    if isinstance(summary, str):
        return summary
    print(f"âš ï¸ [VALIDATION_FAIL] 'summarize' çš„è¼¸å‡ºæ ¼å¼ä¸æ­£ç¢ºï¼Œè¿”å›é è¨­ç¸½çµã€‚æ”¶åˆ°çš„å…§å®¹: {summary}")
    return "å°è©±ç¸½çµè™•ç†éŒ¯èª¤ã€‚"

def run_gpt_prompt_get_recovery_action(persona, mental_state, curr_place):
    action = _safe_llm_call(
        prompt_key="get_recovery_action", prompt_args=[persona, mental_state, curr_place],
        example_output="æª¢æŸ¥æˆ¿å±‹æå‚·",
        special_instruction='è«‹ä»¥åŒ…å«åç‚º "output" çš„å–®ä¸€éµçš„ JSON ç‰©ä»¶æ ¼å¼è¿”å›ã€‚è©²éµçš„å€¼æ‡‰ç‚ºå»ºè­°çš„æ¢å¾©è¡Œå‹•çŸ­èªå­—ç¬¦ä¸²ã€‚',
        repeat=3, fail_safe_json={"output": _DEF_RECOVERY_ACTION}, default_on_error=_DEF_RECOVERY_ACTION
    )
    if isinstance(action, str):
        return action
    print(f"âš ï¸ [VALIDATION_FAIL] 'get_recovery_action' çš„è¼¸å‡ºæ ¼å¼ä¸æ­£ç¢ºï¼Œè¿”å›é è¨­è¡Œå‹•ã€‚æ”¶åˆ°çš„å…§å®¹: {action}")
    return _DEF_RECOVERY_ACTION

def run_gpt_prompt_summarize_disaster(agent_name, mbti, health, experience_log):
    log_str = "\n".join([f"- {entry}" for entry in experience_log])
    if not log_str: log_str = "(æ²’æœ‰å…·é«”äº‹ä»¶è¨˜éŒ„)"

    summary = _safe_llm_call(
        prompt_key="summarize_disaster", prompt_args=[agent_name, mbti, health, log_str],
        example_output="åœ°éœ‡ä¸­å—äº†é»è¼•å‚·ï¼Œä½†å®‰å…¨åº¦éäº†ï¼Œç¾åœ¨æ„Ÿè¦ºæœ‰äº›å¾Œæ€•ã€‚",
        special_instruction='è«‹ä»¥åŒ…å«åç‚º "output" çš„å–®ä¸€éµçš„ JSON ç‰©ä»¶æ ¼å¼è¿”å›ã€‚è©²éµçš„å€¼æ‡‰ç‚ºç°¡çŸ­çš„ç½å¾Œè¨˜æ†¶ç¸½çµå­—ç¬¦ä¸²ã€‚',
        repeat=3, fail_safe_json={"output": _DEF_DISASTER_SUMMARY}, default_on_error=_DEF_DISASTER_SUMMARY
    )
    if isinstance(summary, str):
        return summary
    print(f"âš ï¸ [VALIDATION_FAIL] 'summarize_disaster' çš„è¼¸å‡ºæ ¼å¼ä¸æ­£ç¢ºï¼Œè¿”å›é è¨­ç¸½çµã€‚æ”¶åˆ°çš„å…§å®¹: {summary}")
    return _DEF_DISASTER_SUMMARY
def generate_inner_monologue(agent, eq_ctx=None):
    """
    ç‚ºå–®å€‹ agent ç”Ÿæˆå…§å¿ƒç¨ç™½ã€‚
    """
    earthquake_context = eq_ctx if eq_ctx else "ç›®å‰ä¸€åˆ‡æ­£å¸¸ã€‚"
    now_time = datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')

    prompt_args = [
        agent.name,                     # 0
        agent.MBTI,                     # 1
        agent.personality_desc,         # 2
        agent.curr_place,               # 3
        agent.curr_action,              # 4
        now_time,                       # 5
        agent.memory[-500:],            # 6
        earthquake_context,             # 7
    ]

    example = {
        "thought": "è§’è‰²è¦ºå¾—ç–²æ†Šï¼Œæƒ³è‘—æ™šé£¯åƒä»€éº¼ã€‚",
        "monologue": "ï¼ˆå¥½ç´¯å•Š...ä¸çŸ¥é“ä»Šæ™šè©²åƒé»ä»€éº¼å¥½ã€‚ï¼‰"
    }
    fail_safe = {"output": {"thought": "LLM æ€è€ƒå¤±æ•—ã€‚", "monologue": "ï¼ˆ...ï¼‰"}}

    parsed_output = _safe_llm_call(
        prompt_key="inner_monologue",
        prompt_args=prompt_args,
        example_output=example,
        special_instruction='è«‹ä»¥åŒ…å«åç‚º "output" çš„å–®ä¸€éµçš„ JSON ç‰©ä»¶æ ¼å¼è¿”å›ã€‚è©²éµçš„å€¼æ‡‰ç‚ºä¸€æ®µä»¥ã€Œï¼ˆã€é–‹é ­ï¼Œä»¥ã€Œï¼‰ã€çµå°¾çš„å…§å¿ƒç¨ç™½å­—ç¬¦ä¸²ã€‚',
        repeat=3,
        fail_safe_json=fail_safe,
        default_on_error={"thought": "è§£æéŒ¯èª¤ã€‚", "monologue": "ï¼ˆæ­£åœ¨æ€è€ƒ...ï¼‰"}
    )
    thought = parsed_output.get("thought", "æ€è€ƒéç¨‹æå–å¤±æ•—ã€‚")
    monologue = parsed_output.get("monologue", "ï¼ˆ...ï¼‰")
    if isinstance(monologue, str) and monologue.startswith('ï¼ˆ') and monologue.endswith('ï¼‰'):
        return monologue
    
    print(f"âš ï¸ [VALIDATION_FAIL] 'inner_monologue' çš„è¼¸å‡ºæ ¼å¼ä¸æ­£ç¢ºï¼Œè¿”å›é è¨­ç¨ç™½ã€‚æ”¶åˆ°çš„å…§å®¹: {monologue}")
    return thought, monologue
def run_gpt_prompt_generate_initial_memory(agent):
    """ç‚ºä»£ç†äººç”Ÿæˆåˆå§‹èƒŒæ™¯è¨˜æ†¶å’Œä¸€å€‹ç°¡å–®çš„å¾…è¾¦äº‹é …ã€‚"""
    
    example = {
        "memory": "æœ€è¿‘ä¸€ç›´åœ¨å¿™å·¥ä½œï¼Œæ„Ÿè¦ºæœ‰é»ç´¯ã€‚ä¸Šé€±æœ«å’Œæœ‹å‹å»æµ·é‚Šæ•£æ­¥ï¼Œå¿ƒæƒ…å¥½äº†å¾ˆå¤šã€‚å¸Œæœ›é€™é€±èƒ½æœ‰æ™‚é–“æ”¾é¬†ä¸€ä¸‹ã€‚",
        "schedule": "å¾Œå¤©è¦å»é†«é™¢åšå€‹ä¾‹è¡Œæª¢æŸ¥ã€‚"
    }
    fail_safe = {"output": {"memory": "é€™æ˜¯ä¸€å€‹æ–°é–‹å§‹ã€‚", "schedule": "æ˜å¤©éœ€è¦å»å•†å ´è²·æ±è¥¿ã€‚"}}

    parsed_output = _safe_llm_call(
        prompt_key="generate_initial_memory",
        prompt_args=[agent.name, agent.MBTI, agent.personality_desc, agent.home],
        example_output=example,
        special_instruction='è«‹ä»¥åŒ…å«åç‚º "output" çš„å–®ä¸€éµçš„ JSON ç‰©ä»¶æ ¼å¼è¿”å›ã€‚è©²éµçš„å€¼æ‡‰ç‚ºä¸€å€‹åŒ…å« "memory" å’Œ "schedule" éµçš„ç‰©ä»¶ã€‚è«‹å‹™å¿…ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼ˆTraditional Chineseï¼‰å›ç­”ã€‚',
        repeat=3,
        fail_safe_json=fail_safe,
        default_on_error={"memory": "è¨˜æ†¶ç”Ÿæˆå¤±æ•—ã€‚", "schedule": "æ—¥ç¨‹ç”Ÿæˆå¤±æ•—ã€‚"}
    )

    return parsed_output.get("memory", ""), parsed_output.get("schedule", "")